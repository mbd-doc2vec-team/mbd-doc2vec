\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{url}
\usepackage{hyperref}
\geometry{a4paper}
\geometry{margin=0.75in}

\usepackage[sorting=none, backend=bibtex]{biblatex}
\usepackage{filecontents}
\usepackage{tabularx}

\def\etal{\emph{et al}.\ }
\newcommand{\CPP}
{C\nolinebreak[4]\hspace{-.05em}\raisebox{.22ex}{\footnotesize\bf ++\ }}

\begin{filecontents}{proposal-references.bib}
@misc{wikidatadump2016,
  author="Meta",
  title="Data dump torrents --- Meta{,} discussion about Wikimedia projects",
  year="2016",
  url={https://meta.wikimedia.org/w/index.php?title=Data_dump_torrents},
  note="[Online; accessed 9-August-2016]"
}

@misc{chromium2016,
  title={Git repositories on chromium},
  url={https://chromium.googlesource.com/},
  journal={chromium Git repositories}
}

@InProceedings{maas2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@inproceedings{le2014distributed,
  title={Distributed Representations of Sentences and Documents.},
  author={Le, Quoc V and Mikolov, Tomas},
  booktitle={ICML},
  volume={14},
  pages={1188--1196},
  year={2014}
}

@inproceedings{shvachko2010hadoop,
  title={The hadoop distributed file system},
  author={Shvachko, Konstantin and Kuang, Hairong and Radia, Sanjay and Chansler, Robert},
  booktitle={2010 IEEE 26th symposium on mass storage systems and technologies (MSST)},
  pages={1--10},
  year={2010},
  organization={IEEE}
}

@misc{rnn2015,
  title={The Unreasonable Effectiveness of Recurrent Neural Networks},
  url={https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
  journal={The Unreasonable Effectiveness of Recurrent Neural Networks}
}

@article{shannon2001mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={ACM SIGMOBILE Mobile Computing and Communications Review},
  volume={5},
  number={1},
  pages={3--55},
  year={2001},
  publisher={ACM}
}
\end{filecontents}
\addbibresource{proposal-references.bib}


\title {
  \Huge Mining Big Data - Data Collection Report\\
  \vspace{1em}
  \huge Investigating Paragraph Vectors
}

\author {
  \begin{tabular}{r l}
  a1632538 & Zachary Forman\\
  a1646930 & James Caddy\\
  \end{tabular}
}

\begin{document}

% Cover Page
\pagenumbering{gobble}
\maketitle
\newpage

\subsection*{Data Sources}
  As we discussed in our proposal, we will use three primary data sources.
  Firstly, we use Maas' IMDB set~\cite{maas2011} ((jcad))\\
  Secondly, we used The Wikimedia Foundation's English Wikipedia data dump
  ~\cite{wikidatadump2016} as a source for English language documents. We
  downloaded the full wiki dump from \url{https://dumps.wikimedia.org/enwiki/20160820/}.\\
  Lastly, we used The Chromium Project's Chromium repository~\cite{chromium2016}
  as a source for source code documents. We cloned the repository at revision
  \href{https://github.com/nwjs/chromium.src/commit/4fe31bb06cf458234d7017950a8b2b82427487c8}
       {4fe31bb06cf458234d7017950a8b2b82427487c8}.
\subsection*{Data Scale}
  The IMDB dataset... ((jcad))\\
  The English Wikipedia Dataset is a single XML file, 13GB compressed and
  55GB uncompressed. It contains 16 million pages, however this includes
  redirects and meta-pages such as categories and portals, and therefore only
  5 million of these are useful as documents. Excluding markup, this dataset contains
  approximately 4 billion words.\\
  The Chromium codebase is 2.6GB in size, and contains $245$ thousand files.
  Of these, $23$ thousand are C and \CPP files, which contain $4.8$ million lines of
  code and $608$ thousand lines of comments.
\subsection*{Storage Format}
  The fundamental requirements for the \texttt{doc2vec}~\cite{le2014distributed}
  algorithm to work are that each document can be associated with the words in the
  document, and that words in each document are just a string of words, i.e. no
  excess punctuation or markup.\\
  The simplest way to meet these requirements is to store the data as a series of
  files, one per document, such that each file is named with the document's ID, and
  consists of a series of space separated tokens (words, in the case of English data,
  and code tokens in the case of code data). This advantage, while simple, scales
  well in the distributed case, with distributed filesystems like hdfs~\cite{shvachko2010hadoop}
  providing an easy - and efficient - way to make stored data available over multiple
  nodes.
\subsection*{Data Processing}
  To transform the data into the format previously described, some data processing
  is required. All programs required to transform the datasets are available at
  \url{https://github.com/mbd-doc2vec-team/mbd-doc2vec/tree/master/data-processing}.\\
  The IMDB dataset... ((jcad))\\
  The Wikipedia dataset required extensive pre-processing, with each document
  needing to be parsed from the XML, and the plain text extracted from the wiki markup.
  Additionally, ``fake" documents (e.g. portals, category pages) were removed.\\
  The Chromium codebase required some pre-processing as well, with \CPP files being
  extracted, the directory tree being flattened and the tokens extracted from each file.
\subsection*{Generating Similar Data}
  Given the plethora of sources of natural language data, data generation isn't a high
  priority for us. Perhaps more importantly, simply duplicating given data is a known
  method for increasing the relevance of smaller documents, so to evaluate performance
  on larger datasets we can apply that technique. Despite these reservations, there are
  several promising ways to generate more data. Recurrent Neural Networks (RNNs) show
  great potential as a generative model (that differs from the word vector model used
  in \texttt{doc2vec}) of natural language and source code~\cite{rnn2015}. Similarly,
  Markov chains~\cite{shannon2001mathematical} provide a method of generating more data that
  retains many of the statistical properties of the original corpus.

\newpage
\printbibliography


\end{document}
